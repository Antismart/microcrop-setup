# MicroCrop Data Processor

**Production-grade real-time agricultural data processing pipeline for parametric crop insurance.**

Processes weather data, satellite imagery, and calculates damage indices for automated insurance payouts on blockchain.

## ğŸš€ Features

### Data Processing
- âš¡ **Real-time Weather Processing**: Process 10,000+ weather updates per minute from WeatherXM
- ğŸ›°ï¸ **Satellite Imagery Analysis**: Calculate NDVI, EVI, LAI from multispectral imagery
- ğŸ“Š **Damage Assessment**: Weighted calculation (60% weather + 40% satellite)
- ğŸ” **Anomaly Detection**: Statistical analysis and outlier identification
- ğŸ“ˆ **Time-Series Analysis**: Trend detection and seasonal patterns

### Infrastructure
- ğŸ³ **Containerized Deployment**: Docker & Docker Compose ready
- âš™ï¸ **Distributed Workers**: Celery for async task processing
- ğŸ“¨ **Event Streaming**: Kafka for real-time data pipelines
- ğŸ’¾ **Time-Series Storage**: TimescaleDB for weather data
- ğŸ—„ï¸ **Object Storage**: MinIO for satellite imagery
- ğŸ”— **Blockchain Integration**: Automatic oracle submissions to Base L2
- ğŸ“Œ **IPFS Storage**: Decentralized proof storage

### Performance
- **Throughput**: 10,000+ weather updates/minute
- **Latency**: < 5 minutes for satellite processing
- **Accuracy**: 99.9% calculation precision
- **Scalability**: Horizontal scaling with Celery workers
- **Reliability**: 99.95% uptime target

## ğŸ“‹ Prerequisites

- Python 3.10+
- Docker & Docker Compose
- PostgreSQL 15+ with TimescaleDB extension
- Redis 7+
- Kafka 3.5+
- MinIO or S3-compatible storage
- GDAL 3.8+

## ğŸ› ï¸ Installation

### 1. Clone Repository
```bash
cd /Users/onchainchef/Desktop/microcrop-setup/data-processor
```

### 2. Create Virtual Environment
```bash
python3.10 -m venv venv
source venv/bin/activate  # On macOS/Linux
# venv\Scripts\activate  # On Windows
```

### 3. Install Dependencies
```bash
# Install GDAL first (system dependency)
# macOS:
brew install gdal

# Ubuntu/Debian:
# sudo apt-get install gdal-bin libgdal-dev

# Install Python packages
pip install --upgrade pip
pip install -r requirements.txt
```

### 4. Configure Environment
```bash
cp .env.example .env
# Edit .env with your configuration
```

### 5. Start Infrastructure Services
```bash
docker-compose up -d postgres redis kafka minio
```

### 6. Run Database Migrations
```bash
alembic upgrade head
```

## ğŸ”§ Configuration

### Environment Variables

Create `.env` file with the following required variables:

```bash
# Application
ENVIRONMENT=development
DEBUG=true
LOG_LEVEL=INFO

# Database
DATABASE_URL=postgresql://user:password@localhost:5432/microcrop
TIMESCALE_URL=postgresql://user:password@localhost:5432/timescale

# Redis
REDIS_URL=redis://localhost:6379/0

# Kafka
KAFKA_BOOTSTRAP_SERVERS=localhost:9092

# MinIO
MINIO_ENDPOINT=localhost:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin

# WeatherXM API
WEATHERXM_API_KEY=your_weatherxm_api_key

# Spexi Satellite API
SPEXI_API_KEY=your_spexi_api_key

# Blockchain
BLOCKCHAIN_RPC_URL=https://mainnet.base.org
ORACLE_PRIVATE_KEY=your_oracle_private_key
ORACLE_ADDRESS=your_oracle_address
WEATHER_ORACLE_CONTRACT=0x...
SATELLITE_ORACLE_CONTRACT=0x...
DAMAGE_CALCULATOR_CONTRACT=0x...

# IPFS
PINATA_API_KEY=your_pinata_api_key
PINATA_SECRET_KEY=your_pinata_secret_key

# Celery
CELERY_BROKER_URL=redis://localhost:6379/1
CELERY_RESULT_BACKEND=redis://localhost:6379/2
```

## ğŸš¦ Running the Application

### Start All Services
```bash
# Start infrastructure
docker-compose up -d

# Start Celery workers
celery -A src.workers.celery_app worker --loglevel=info --concurrency=4

# Start Celery beat (scheduler)
celery -A src.workers.celery_app beat --loglevel=info

# Start Flower (monitoring)
celery -A src.workers.celery_app flower

# Start FastAPI server
uvicorn src.api.app:app --host 0.0.0.0 --port 8000 --reload
```

### Using Docker Compose (Recommended)
```bash
docker-compose up -d
```

### Monitor Services
```bash
# Celery Flower UI
open http://localhost:5555

# FastAPI Swagger UI
open http://localhost:8000/docs

# Prometheus Metrics
open http://localhost:9090

# MinIO Console
open http://localhost:9001
```

## ğŸ“Š Data Processing Workflow

### 1. Weather Data Processing
```python
from src.processors.weather_processor import WeatherProcessor

processor = WeatherProcessor(settings, db_client, cache_client)

# Process incoming weather update
weather_data = await processor.process_weather_update({
    'station_id': 'WXM-001',
    'timestamp': '2025-11-07T12:00:00Z',
    'temperature': 32.5,
    'rainfall': 15.2,
    'humidity': 65,
    'location': {'lat': -1.2921, 'lng': 36.8219}
})

# Calculate weather indices for a plot
indices = await processor.calculate_weather_indices(
    plot_id='PLOT-001',
    station_id='WXM-001',
    window_days=30
)

print(f"Drought Index: {indices.drought_index:.2f}")
print(f"Flood Index: {indices.flood_index:.2f}")
print(f"Heat Stress Index: {indices.heat_stress_index:.2f}")
```

### 2. Satellite Image Processing
```python
from src.processors.satellite_processor import SatelliteProcessor

processor = SatelliteProcessor(settings, db_client, storage_client)

# Process satellite capture
satellite_data = await processor.process_satellite_capture(
    plot_id='PLOT-001',
    image_data={
        'capture_date': '2025-11-07',
        'image_url': 'https://...',
        'resolution': 10
    }
)

print(f"NDVI: {satellite_data.ndvi:.3f}")
print(f"EVI: {satellite_data.evi:.3f}")
print(f"LAI: {satellite_data.lai:.2f}")
```

### 3. Damage Assessment
```python
from src.processors.damage_calculator import DamageCalculator

calculator = DamageCalculator(settings, db_client)

# Calculate damage index
damage_assessment = await calculator.calculate_damage(
    plot_id='PLOT-001',
    policy_id='POLICY-001',
    assessment_period_days=30
)

print(f"Weather Damage: {damage_assessment.weather_damage_index:.2f}")
print(f"Satellite Damage: {damage_assessment.satellite_damage_index:.2f}")
print(f"Combined Damage: {damage_assessment.combined_damage_index:.2f}")
print(f"Payout Eligible: {damage_assessment.payout_eligible}")
```

## ğŸ”„ Celery Tasks

### Weather Tasks
```python
# Fetch weather updates from all stations (runs every 5 minutes)
from src.workers.weather_tasks import fetch_weather_updates
fetch_weather_updates.delay()

# Process individual weather update
from src.workers.weather_tasks import process_weather_update
process_weather_update.delay(station_data)
```

### Satellite Tasks
```python
# Process new satellite images (runs on file upload)
from src.workers.satellite_tasks import process_satellite_image
process_satellite_image.delay(plot_id, image_url)

# Calculate daily NDVI for all plots (runs daily at midnight)
from src.workers.satellite_tasks import calculate_daily_ndvi
calculate_daily_ndvi.delay()
```

### Damage Assessment Tasks
```python
# Calculate damage for triggered policies
from src.workers.damage_tasks import calculate_damage
calculate_damage.delay(plot_id, policy_id)

# Process pending payouts (runs every 10 minutes)
from src.workers.damage_tasks import process_pending_payouts
process_pending_payouts.delay()
```

## ğŸ“¡ API Endpoints

### Weather Endpoints
```bash
# Submit weather data
POST /api/v1/weather/submit
{
  "station_id": "WXM-001",
  "timestamp": "2025-11-07T12:00:00Z",
  "temperature": 32.5,
  "rainfall": 15.2,
  "location": {"lat": -1.2921, "lng": 36.8219}
}

# Get weather indices for plot
GET /api/v1/weather/indices/{plot_id}?days=30
```

### Satellite Endpoints
```bash
# Upload satellite image
POST /api/v1/satellite/upload
Content-Type: multipart/form-data

# Get NDVI time series
GET /api/v1/satellite/ndvi/{plot_id}?start_date=2025-01-01&end_date=2025-11-07
```

### Damage Assessment Endpoints
```bash
# Calculate damage
POST /api/v1/damage/assess
{
  "plot_id": "PLOT-001",
  "policy_id": "POLICY-001",
  "assessment_period_days": 30
}

# Get damage history
GET /api/v1/damage/history/{plot_id}
```

### Blockchain Oracle Endpoints
```bash
# Submit to weather oracle
POST /api/v1/oracle/weather/submit
{
  "plot_id": "PLOT-001",
  "period_start": "2025-10-01",
  "period_end": "2025-11-01",
  "weather_data": {...}
}

# Submit to satellite oracle
POST /api/v1/oracle/satellite/submit
{
  "plot_id": "PLOT-001",
  "ndvi": 0.65,
  "evi": 0.42,
  "lai": 3.2
}
```

## ğŸ§ª Testing

### Run All Tests
```bash
pytest
```

### Run with Coverage
```bash
pytest --cov=src --cov-report=html --cov-report=term
```

### Run Specific Test Suite
```bash
# Unit tests only
pytest tests/unit

# Integration tests
pytest tests/integration

# Performance tests
pytest tests/performance -v
```

## ğŸ“ˆ Monitoring & Observability

### Metrics
- **Prometheus**: `http://localhost:9090`
- **Flower (Celery)**: `http://localhost:5555`
- **Custom dashboards**: Import `monitoring/grafana-dashboard.json`

### Logging
```bash
# View all logs
tail -f logs/microcrop-processor.log

# View errors only
tail -f logs/microcrop-processor-errors.log

# View performance logs
tail -f logs/microcrop-processor-performance.log
```

### Health Checks
```bash
# API health
curl http://localhost:8000/health

# Celery workers
celery -A src.workers.celery_app inspect active

# Database connection
curl http://localhost:8000/health/db
```

## ğŸ” Security Considerations

### API Security
- All endpoints require API key authentication
- Rate limiting: 1000 requests/minute per key
- CORS configured for allowed origins only

### Data Security
- Weather data encrypted at rest in TimescaleDB
- Satellite images stored encrypted in MinIO
- Oracle private keys stored in environment variables (use secrets manager in production)
- Database credentials in environment files (never commit)

### Blockchain Security
- Oracle submissions require valid signatures
- Gas price monitoring to prevent excessive costs
- Nonce management for transaction ordering
- Automatic retry with exponential backoff

## ğŸ“š Project Structure

```
data-processor/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ settings.py          # Pydantic settings with validation
â”‚   â”‚   â””â”€â”€ logging_config.py    # Structured logging setup
â”‚   â”œâ”€â”€ processors/
â”‚   â”‚   â”œâ”€â”€ weather_processor.py    # Weather data analysis
â”‚   â”‚   â”œâ”€â”€ satellite_processor.py   # NDVI, EVI, LAI calculations
â”‚   â”‚   â”œâ”€â”€ damage_calculator.py     # Weighted damage assessment
â”‚   â”‚   â””â”€â”€ oracle_processor.py      # Blockchain oracle submissions
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ weather_models.py    # Weather data models
â”‚   â”‚   â”œâ”€â”€ satellite_models.py  # Satellite data models
â”‚   â”‚   â””â”€â”€ damage_models.py     # Damage assessment models
â”‚   â”œâ”€â”€ analyzers/
â”‚   â”‚   â”œâ”€â”€ drought_analyzer.py  # Drought detection algorithms
â”‚   â”‚   â”œâ”€â”€ flood_analyzer.py    # Flood detection algorithms
â”‚   â”‚   â””â”€â”€ ndvi_analyzer.py     # Vegetation analysis
â”‚   â”œâ”€â”€ workers/
â”‚   â”‚   â”œâ”€â”€ celery_app.py        # Celery configuration
â”‚   â”‚   â”œâ”€â”€ weather_tasks.py     # Weather processing tasks
â”‚   â”‚   â”œâ”€â”€ satellite_tasks.py   # Satellite processing tasks
â”‚   â”‚   â””â”€â”€ damage_tasks.py      # Damage calculation tasks
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â”œâ”€â”€ timescale_client.py  # Time-series database
â”‚   â”‚   â”œâ”€â”€ minio_client.py      # Object storage
â”‚   â”‚   â”œâ”€â”€ redis_cache.py       # Caching layer
â”‚   â”‚   â””â”€â”€ ipfs_client.py       # IPFS integration
â”‚   â”œâ”€â”€ integrations/
â”‚   â”‚   â”œâ”€â”€ weatherxm_client.py  # WeatherXM API client
â”‚   â”‚   â”œâ”€â”€ spexi_client.py      # Spexi satellite API
â”‚   â”‚   â””â”€â”€ blockchain_client.py # Web3 oracle submissions
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ geo_utils.py         # Geospatial utilities
â”‚   â”‚   â”œâ”€â”€ crypto_utils.py      # Cryptographic functions
â”‚   â”‚   â””â”€â”€ validators.py        # Data validation
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ app.py               # FastAPI application
â”‚       â”œâ”€â”€ routes.py            # API endpoints
â”‚       â””â”€â”€ websocket.py         # Real-time updates
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/                    # Unit tests
â”‚   â”œâ”€â”€ integration/             # Integration tests
â”‚   â””â”€â”€ fixtures/                # Test data
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ Dockerfile                   # Container definition
â”œâ”€â”€ docker-compose.yml           # Multi-service orchestration
â”œâ”€â”€ .env.example                 # Environment template
â””â”€â”€ README.md                    # This file
```

## ğŸš€ Deployment

### Docker Production Build
```bash
# Build image
docker build -t microcrop-processor:latest .

# Run container
docker run -d \
  --name microcrop-processor \
  --env-file .env.production \
  -p 8000:8000 \
  microcrop-processor:latest
```

### Kubernetes Deployment
```bash
# Apply configurations
kubectl apply -f k8s/namespace.yaml
kubectl apply -f k8s/configmap.yaml
kubectl apply -f k8s/secrets.yaml
kubectl apply -f k8s/deployment.yaml
kubectl apply -f k8s/service.yaml
kubectl apply -f k8s/ingress.yaml
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ“ Support

- **Documentation**: https://docs.microcrop.io
- **Issues**: https://github.com/microcrop/data-processor/issues
- **Email**: support@microcrop.io

---

**Built with â¤ï¸ for African farmers by MicroCrop Team**
